{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add your system path here\n",
    "sys_path = r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\PowerPrediction\\ml-project-2-powerpredictors'\n",
    "sys.path.append(sys_path)\n",
    "\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009652509652509653"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = sys_path + r'\\data\\dataset_v2.csv'\n",
    "data = LoadData(path)\n",
    "\n",
    "# Showing the relative amount of data that are Nan in a column\n",
    "relnan(data, 'wind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking which sections are Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_sections = return_nan_sections(data, 'ghi') # Returns array with the first and last index of the data frame where the values are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_sections = return_nan_sections(data, 'total_p_demand [kW]') # Returns array with the first and last index of the data frame where the values are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>total_p_demand [kW]</th>\n",
       "      <th>ghi</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, month, day, hour, total_p_demand [kW], ghi, temp, wind]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the outcome\n",
    "nan_n = 0\n",
    "data[data.keys()][nan_sections[nan_n][0]-:nan_sections[nan_n][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>total_p_demand [kW]</th>\n",
       "      <th>ghi</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24859</th>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24860</th>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24861</th>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24862</th>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24863</th>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24864 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  hour  total_p_demand [kW]  ghi  temp  wind\n",
       "0      2022      1    1     0                  0.0  0.0  4.12  1.10\n",
       "1      2022      1    1     1                  0.0  0.0  3.92  1.31\n",
       "2      2022      1    1     2                  0.0  0.0  3.67  1.24\n",
       "3      2022      1    1     3                  0.0  0.0  3.51  1.17\n",
       "4      2022      1    1     4                  0.0  0.0  3.25  1.17\n",
       "...     ...    ...  ...   ...                  ...  ...   ...   ...\n",
       "24859  2024     11    1    19                  0.0  0.0  0.00  0.00\n",
       "24860  2024     11    1    20                  0.0  0.0  0.00  0.00\n",
       "24861  2024     11    1    21                  0.0  0.0  0.00  0.00\n",
       "24862  2024     11    1    22                  0.0  0.0  0.00  0.00\n",
       "24863  2024     11    1    23                  0.0  0.0  0.00  0.00\n",
       "\n",
       "[24864 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna(0) # Fill Nans with 0 for intermediary purpose\n",
    "data = torch.from_numpy(data.to_numpy()) # Convert to torch tensor\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here its included one element before and behind the returned indices to check that it worked, which it did. Now I am happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features and targets to train the network\n",
    "Here we will cut the whole data in slices of 7 days, which will be the features. \\\n",
    "The value of the power for the first hour of the 8th day should be the target. \\\n",
    "Then we will save them as features and targets to use them with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, datasets, seq_len=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            datasets (list of numpy.ndarray): List of time series datasets, \n",
    "                each of shape (n_hours, n_features).\n",
    "            seq_len (int): Length of the input sequence (1 for hour-by-hour training).\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        for data in datasets:\n",
    "            for i in range(len(data) - seq_len):\n",
    "                # Create input-output pairs for each dataset\n",
    "                x = data[i:i + seq_len]\n",
    "                y = data[i + seq_len]\n",
    "                self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0220e+03, 1.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 4.1200e+00,\n",
       "         1.1000e+00],\n",
       "        [2.0220e+03, 1.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 3.9200e+00,\n",
       "         1.3100e+00],\n",
       "        [2.0220e+03, 1.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 3.6700e+00,\n",
       "         1.2400e+00],\n",
       "        ...,\n",
       "        [2.0240e+03, 1.1000e+01, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [2.0240e+03, 1.1000e+01, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [2.0240e+03, 1.1000e+01, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming datasets is a list of numpy arrays\n",
    "multi_dataset = MultiTimeSeriesDataset(data)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(multi_dataset))\n",
    "val_size = len(multi_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(multi_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM-based regression model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features (e.g., temperature, GHI, etc.).\n",
    "            hidden_size (int): Number of units in each LSTM layer.\n",
    "            num_layers (int): Number of stacked LSTM layers.\n",
    "            output_size (int): Number of output features (e.g., predicted demand, 1 for regression).\n",
    "            dropout_prob (float): Dropout probability to apply between LSTM layers and before the fully connected layer.\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # LSTM Layer\n",
    "        # - Processes sequential data and learns temporal dependencies.\n",
    "        # - Supports multiple layers (num_layers) and applies dropout between layers.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,  # Input/output shape: (batch_size, seq_length, input_size)\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully Connected (Linear) Layer\n",
    "        # - Maps the LSTM's hidden state output to the desired output size.\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Dropout Layer\n",
    "        # - Reduces overfitting by randomly zeroing some activations during training.\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the LSTM model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # LSTM Layer\n",
    "        # - Returns the full sequence of hidden states and the final hidden/cell state tuple.\n",
    "        # - We ignore the hidden/cell state tuple here (h_n, c_n).\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Dropout Layer\n",
    "        # - Only uses the hidden state from the last time step for prediction.\n",
    "        # - Applies dropout to prevent overfitting.\n",
    "        out = self.dropout(out[:, -1, :])  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        # - Maps the LSTM's output to the desired output size (e.g., single regression output).\n",
    "        out = self.fc(out)  # Shape: (batch_size, output_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max Tost\\AppData\\Local\\Temp\\ipykernel_5640\\119286156.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 3, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Reset hidden state between sequences\u001b[39;00m\n\u001b[0;32m     16\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Allows LSTM to initialize its hidden state\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlstm(x, hidden)\n\u001b[0;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Take last output for regression\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1100\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m   1094\u001b[0m         max_batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m   1098\u001b[0m     )\n\u001b[0;32m   1099\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[1;32m-> 1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[0;32m    999\u001b[0m ):\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1010\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 3, got 1"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=3, hidden_size=64, num_layers=2, output_size=3, dropout_prob=0.2)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Reset hidden state between sequences\n",
    "        hidden = None  # Allows LSTM to initialize its hidden state\n",
    "        output, hidden = model.lstm(x, hidden)\n",
    "        output = model.fc(output[:, -1, :])  # Take last output for regression\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            output, _ = model.lstm(x_val, None)  # Reset hidden state for validation\n",
    "            val_output = model.fc(output[:, -1, :])\n",
    "            val_loss += criterion(val_output, y_val).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, initial_data, num_predictions=24):\n",
    "    \"\"\"\n",
    "    Evaluate the LSTM to predict the next 24 hours recursively.\n",
    "\n",
    "    Args:\n",
    "        model (LSTMModel): Trained LSTM model.\n",
    "        initial_data (torch.Tensor): Data from the last 7 days (shape: [168, num_features]).\n",
    "        num_predictions (int): Number of hours to predict (default: 24).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicted values for the next 24 hours (shape: [24, num_features]).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    # Initialize hidden state with the last 7 days\n",
    "    with torch.no_grad():\n",
    "        input_seq = initial_data.unsqueeze(0)  # Shape: [1, seq_len=168, num_features]\n",
    "        hidden = None  # Let the LSTM initialize hidden state\n",
    "\n",
    "        # Process the last 7 days to initialize hidden state\n",
    "        for t in range(initial_data.size(0)):\n",
    "            _, hidden = model.lstm(input_seq[:, t:t+1, :], hidden)\n",
    "\n",
    "        # Recursive prediction for the next 24 hours\n",
    "        last_input = initial_data[-1, :].unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, num_features]\n",
    "        for _ in range(num_predictions):\n",
    "            output, hidden = model.lstm(last_input, hidden)  # Predict next hour\n",
    "            prediction = model.fc(output[:, -1, :])  # Map hidden state to output\n",
    "            predictions.append(prediction.squeeze(0))\n",
    "\n",
    "            # Use the predicted value as the next input\n",
    "            last_input = prediction.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    return torch.stack(predictions)  # Shape: [24, num_features]\n",
    "\n",
    "# Example usage\n",
    "last_week_data = torch.tensor(data[-168:], dtype=torch.float32)  # Last 7 days of data\n",
    "predictions = evaluate_model(model, last_week_data)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_predictions(model, x, n_simulations):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo Dropout predictions to estimate both \n",
    "    the mean prediction and uncertainty.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained PyTorch model with dropout layers.\n",
    "        x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_features).\n",
    "        n_simulations (int): Number of stochastic forward passes to perform.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - mean_pred (torch.Tensor): The mean prediction across all simulations.\n",
    "              Shape: (batch_size, output_features).\n",
    "            - uncertainty (torch.Tensor): The standard deviation of predictions \n",
    "              (representing uncertainty) across simulations.\n",
    "              Shape: (batch_size, output_features).\n",
    "    \"\"\"\n",
    "    # Set the model to train mode to enable dropout during inference\n",
    "    # Dropout layers behave stochastically in train mode, which is necessary for Monte Carlo sampling\n",
    "    model.train()\n",
    "\n",
    "    # Perform n_simulations stochastic forward passes\n",
    "    # Each simulation generates slightly different predictions due to dropout\n",
    "    preds = torch.stack([model(x) for _ in range(n_simulations)])  # Shape: (n_simulations, batch_size, output_features)\n",
    "\n",
    "    # Compute the mean prediction across all simulations\n",
    "    mean_pred = preds.mean(dim=0)  # Shape: (batch_size, output_features)\n",
    "\n",
    "    # Compute the standard deviation across simulations to estimate uncertainty\n",
    "    uncertainty = preds.std(dim=0)  # Shape: (batch_size, output_features)\n",
    "\n",
    "    return mean_pred, uncertainty\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
