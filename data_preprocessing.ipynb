{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6406ae-5b1a-4aae-bc2d-2847bb41608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f8f5c7-7818-4a06-aafe-bd41fb868f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import prepare_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f579c10-2b2f-4040-9883-f692d5146718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Set print options to suppress scientific notation and show 3 decimal places\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "# Suppress all warnings globally\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb53d74-6249-4bfa-a23a-f47b5a8fc76f",
   "metadata": {},
   "source": [
    "### Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c870ca-6a01-484f-bf3a-e88478783d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = 0\n",
    "if augmented_dataset:\n",
    "    file_path = os.path.join(current_directory, 'data_augmented/X_augmented.csv')\n",
    "else:\n",
    "    file_path = os.path.join(current_directory, 'data_augmented/X.csv')\n",
    "\n",
    "X = pd.read_csv(file_path, index_col = 0)\n",
    "df = X \n",
    "\n",
    "file_path = os.path.join(current_directory, 'data_augmented/timestamps.csv')\n",
    "timestamps = pd.read_csv(file_path, index_col = 0)\n",
    "\n",
    "df['timestamp'] = timestamps\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.asfreq('H')  # 'H' for hourly frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5fe4c-bed6-4552-af76-08d38cde7d63",
   "metadata": {},
   "source": [
    "### Train-Test split\n",
    "\n",
    "In the context of time series forecasting for electricity market bidding, it is crucial to split the dataset into training and testing sets in a way that aligns with real-world operational requirements. Here, the test set is designed to reflect the market dynamics, where bidding occurs daily at 10 AM for the next 24 hours. \n",
    "\n",
    "To achieve this, the code ensures:\n",
    "1. The test set starts at the first available 11 AM, enabling the model to forecast the next 24 hours based on data available just before the bidding deadline.\n",
    "2. The test set ends at the last available 10 AM, ensuring predictions align precisely with the actual bidding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958475a4-5498-4bda-bc84-bc3102f20f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "train_test_split_ratio = 0.95\n",
    "train_size = int(len(df) * train_test_split_ratio)  \n",
    "initial_test_start = train_size  \n",
    "\n",
    "while df.index[initial_test_start].hour != 11: # Adjust test start to align with the next occurrence of 11 AM\n",
    "    initial_test_start += 1\n",
    "\n",
    "final_test_end = len(df) - 1\n",
    "while df.index[final_test_end].hour != 10: # Adjust test end to align with the last 10 AM in the dataset\n",
    "    final_test_end -= 1\n",
    "\n",
    "train = df.iloc[:initial_test_start]\n",
    "test = df.iloc[initial_test_start:final_test_end+1]  # Include the last index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12f0584-9620-4a80-aa6b-0a57e2f61285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimensions:\n",
      "(18145, 32)\n",
      "Test set dimensions:\n",
      "(936, 32)\n"
     ]
    }
   ],
   "source": [
    "print('Train set dimensions:')\n",
    "print(train.shape) # (num_train_samples, num_features)\n",
    "print('Test set dimensions:')\n",
    "print(test.shape) # (num_test_samples, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c1ba6-3819-4124-9664-810b80cdaadf",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Training and test data are standardized separately to prevent **data leakage**, ensuring the test set remains independent from the training process. By doing so, the model’s performance reflects real-world scenarios where unseen data is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1863e21f-503a-4eab-b11b-516f67c3f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if augmented_dataset:\n",
    "    columns_not_to_scale = ['is_monday', 'is_tuesday', 'is_wednesday',\n",
    "           'is_thursday', 'is_friday', 'is_saturday', 'is_sunday', 'is_weekend',\n",
    "           'is_spring', 'is_summer', 'is_autumn', 'is_winter', 'is_holiday',\n",
    "           'is_daylight','hour_1', 'hour_2', 'hour_3',\n",
    "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9',\n",
    "       'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15',\n",
    "       'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21',\n",
    "       'hour_22', 'hour_23', 'month_2', 'month_3', 'month_4', 'month_5',\n",
    "       'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n",
    "       'month_12']\n",
    "    columns_to_scale = [col for col in train.columns if col not in columns_not_to_scale]\n",
    "else:\n",
    "    columns_to_scale = ['power_consumption', 'temp']\n",
    "\n",
    "# Train data\n",
    "scaler_train = StandardScaler()\n",
    "scaled_train = pd.DataFrame(\n",
    "    scaler_train.fit_transform(train[columns_to_scale]),\n",
    "    columns=columns_to_scale\n",
    ")\n",
    "\n",
    "means_train = pd.DataFrame(columns = columns_to_scale)\n",
    "means_train.loc[0] = scaler_train.mean_\n",
    "stds_train = pd.DataFrame(columns = columns_to_scale)\n",
    "stds_train.loc[0] = scaler_train.scale_\n",
    "\n",
    "train[columns_to_scale] = scaled_train.values\n",
    "\n",
    "# Test data\n",
    "scaler_test = StandardScaler()\n",
    "scaled_test = pd.DataFrame(\n",
    "    scaler_test.fit_transform(test[columns_to_scale]),\n",
    "    columns=columns_to_scale\n",
    ")\n",
    "\n",
    "means_test = pd.DataFrame(columns = columns_to_scale)\n",
    "means_test.loc[0] = scaler_test.mean_\n",
    "stds_test = pd.DataFrame(columns = columns_to_scale)\n",
    "stds_test.loc[0] = scaler_test.scale_\n",
    "\n",
    "test[columns_to_scale] = scaled_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc91b42-30d0-4fea-b3d1-f78c7624d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test data\n",
    "file_path = os.path.join(current_directory, 'data_augmented/train.csv')\n",
    "train.to_csv(file_path)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/test.csv')\n",
    "test.to_csv(file_path)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/means_train.csv')\n",
    "means_train.to_csv(file_path)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/means_test.csv')\n",
    "means_test.to_csv(file_path)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/stds_train.csv')\n",
    "stds_train.to_csv(file_path)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/stds_test.csv')\n",
    "stds_test.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74925a04-9bb5-4fd2-b74f-ac13c9274b95",
   "metadata": {},
   "source": [
    "### Pre-process data for LSTM and TCN Models\n",
    "\n",
    "When training temporal models like LSTMs or TCNs, the data must be structured into sequences to capture temporal dependencies effectively. The function `prepare_features` preprocesses the data by creating feature tensors (`X`) and target tensors (`y`) suitable for training these models. Additionally, the function allows for the inclusion of forecasted exogenous variables (e.g., temperature or wind forecasts) alongside historical data, ensuring that the model can utilize all relevant information available at prediction time. Missing values are handled using forward and backward filling to ensure data consistency without losing samples.\n",
    "\n",
    "\n",
    "If forecasted variables are included, the function generates new columns for each forecasted variable, representing hourly predictions over the forecast horizon (e.g., `temp_forecast_1h`, `temp_forecast_2h`, ..., `temp_forecast_24h`). These columns are created by shifting the original exogenous variable backward in time to align forecasted features with their corresponding prediction intervals. This allows the model to incorporate known forecasted inputs when predicting power consumption for the next 24 hours.\n",
    "\n",
    "##### Structure of `X` and `y` Tensors\n",
    "\n",
    "1. **Feature Tensor (`X`)**:\n",
    "   - **Shape**: `(num_samples, window_length, num_features)`\n",
    "   - **Content**: \n",
    "     - For each sequence, the historical window (`window_length`) includes the target variable (e.g., power consumption) and exogenous variables (e.g., temperature).\n",
    "     - If forecasted variables are included, each sequence also contains the aligned forecasted values for the entire horizon.\n",
    "\n",
    "2. **Target Tensor (`y`)**:\n",
    "   - **Shape**: `(num_samples, forecast_horizon)`\n",
    "   - **Content**: The target tensor contains the power consumption values for the next 24 hours for each sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99162550-256d-495a-9bbd-7632b6b47690",
   "metadata": {},
   "source": [
    "The choice of **168 hours** as the sequence length corresponds to one full week of historical data, allowing the model to capture **weekly seasonal patterns** such as differences between weekdays and weekends, as well as weather-driven fluctuations. This length provides sufficient context to detect **trends, seasonal fluctuations, and anomalies** while balancing **information richness and computational efficiency**. Longer sequences could increase computational cost with diminishing returns, making 168 hours an optimal choice for many applications. Additionally, it aligns with practical considerations in energy markets, where weekly cycles are significant in forecasting power consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a95c7a3-26a5-49b0-a291-61dcb4da48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data to NN: no forecasted variables included\n",
    "include_forecast = False\n",
    "\n",
    "target_col = 'power_consumption'\n",
    "forecast_cols = ['temp'] # Choose between: ['temp'], [col for col in df.columns if col not in target_col]\n",
    "window_length = 168  # 7 days\n",
    "forecast_horizon = 24  # 24 hours\n",
    "\n",
    "X_train, y_train, timestamps_train = prepare_features(train, target_col, forecast_cols, window_length, forecast_horizon, include_forecast)\n",
    "X_test, y_test, timestamps_test = prepare_features(test, target_col, forecast_cols, window_length, forecast_horizon, include_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382dade4-82f5-4fcc-b29d-38a7af8c69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save input data to NN\n",
    "file_path = os.path.join(current_directory, 'data_augmented/X_train.npy')\n",
    "np.save(file_path, X_train)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/y_train.npy')\n",
    "np.save(file_path, y_train)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/timestamps_train.csv')\n",
    "timestamps_train.to_series().to_csv(file_path, index=False) \n",
    "file_path = os.path.join(current_directory, 'data_augmented/X_test.npy')\n",
    "np.save(file_path, X_test)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/y_test.npy')\n",
    "np.save(file_path, y_test)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/timestamps_test.csv')\n",
    "timestamps_test.to_series().to_csv(file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ebaa76-ffd2-4575-a82b-e00ca27c4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data to NN: forecasted variables included\n",
    "include_forecast = True\n",
    "\n",
    "X_train, y_train, timestamps_train = prepare_features(train, target_col, forecast_cols, window_length, forecast_horizon, include_forecast)\n",
    "X_test, y_test, timestamps_test = prepare_features(test, target_col, forecast_cols, window_length, forecast_horizon, include_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5872b312-35fa-465c-941d-bde783a0537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save input data to NN\n",
    "file_path = os.path.join(current_directory, 'data_augmented/X_train_include_forecast.npy')\n",
    "np.save(file_path, X_train)\n",
    "file_path = os.path.join(current_directory, 'data_augmented/X_test_include_forecast.npy')\n",
    "np.save(file_path, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc334315-e33d-4a9c-938e-28d014100573",
   "metadata": {},
   "source": [
    "### Metrics to Evaluate Prediction Accuracy\n",
    "\n",
    "The evaluation of prediction accuracy is performed with multiple metrics, as each highlights different aspects of model performance. \n",
    "\n",
    "1. **Root Mean Squared Error (RMSE)**  \n",
    "   $$\n",
    "   \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2}\n",
    "   $$\n",
    "   - Sensitive to large errors (penalizes outliers).  \n",
    "   - Captures the overall accuracy but is non-robust to extreme values.\n",
    "     \n",
    "\n",
    "2. **Mean Absolute Error (MAE)**  \n",
    "   $$\n",
    "   \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |\\hat{y}_i - y_i|\n",
    "   $$  \n",
    "   - Treats all errors equally.  \n",
    "   - Provides a robust measure of the average error magnitude.\n",
    "\n",
    "  \n",
    "3. **Maximum Error (ME)**  \n",
    "   $$\n",
    "   \\text{ME} = \\max(|\\hat{y}_i - y_i|)\n",
    "   $$  \n",
    "   - Highlights the largest deviation, capturing extreme behavior.\n",
    "\n",
    "  \n",
    "4. **Mean Absolute Percentage Error (MAPE)**  \n",
    "   $$\n",
    "   \\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^n \\left| \\frac{\\hat{y}_i - y_i}{y_i} \\right|\n",
    "   $$  \n",
    "   - Expresses errors as a percentage, providing relative interpretability.  \n",
    "   - Sensitive to small actual values.\n",
    "\n",
    "These metrics together offer a holistic evaluation of a model, balancing overall accuracy, robustness, and sensitivity to extreme deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f445c975-e7b3-4366-ad03-45125657d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame(columns = ['RMSE', 'MAE', 'ME', 'MAPE'])\n",
    "file_path = os.path.join(current_directory, 'results/errors.csv')\n",
    "errors.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525cea5-b173-4027-b2b9-80f0b377f661",
   "metadata": {},
   "source": [
    "### Metrics to evaluate uncertainty of predictions \n",
    "\n",
    "The predictions uncertainty is quantified with the metrics PICP and PINAW. \n",
    "\n",
    "1. **Prediction Interval Coverage Probability (PICP)**:\n",
    "   - Measures the percentage of true values captured within the prediction intervals.\n",
    "   - A high PICP (e.g., 95%) indicates well-calibrated intervals.\n",
    "   - Too low: Intervals miss actual values (underestimating uncertainty).\n",
    "   - Too high: Intervals are overly wide (too conservative).\n",
    "\n",
    "2. **Prediction Interval Normalized Average Width (PINAW)**:\n",
    "   - Quantifies the average width of prediction intervals relative to the range of true values.\n",
    "   - A low PINAW means the intervals are narrow (sharp).\n",
    "   - Too narrow: Risk of poor coverage (low PICP).\n",
    "   - Too wide: Excessive conservatism, reducing utility.\n",
    "\n",
    "The goal is to balance **PICP** (coverage) and **PINAW** (sharpness) to create reliable and efficient prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db61df5e-e967-47eb-ab4d-c403b613008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_evaluation = pd.DataFrame(columns = ['PICP', 'PINAW'])\n",
    "file_path = os.path.join(current_directory, 'results/uncertainty_evaluation.csv')\n",
    "uncertainty_evaluation.to_csv(file_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
